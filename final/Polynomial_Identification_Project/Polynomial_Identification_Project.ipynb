{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* Name 1: \n",
    "* Name 2:\n",
    "\n",
    "## To be submitted by: 15.3.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomials Identification Project\n",
    "\n",
    "This project identifies poynomials given in images. Specifically, polynomials of rank 3 are provided as RGB images to a neural netork, whose outputs are the coefficients of the given polynomials. The domain considered is \"[0,1]x[0,1] box\". PyTorch is the platform used.\n",
    "\n",
    "### **You are given the following files:**\n",
    "  Three directories named \"train\", \"validation\", and \"test\" each containing the following:\n",
    "  1. a directory called \"images\" with RGB png files of size 128x128 (so image that is read has shape (3,128,128))\n",
    "  2. a file \"labels.txt\" containing the coefficients of all images in \"images\" directory\n",
    "\n",
    "Since this problem is a somewhat open ended, you are given a suggested structure and some functionality that is already implemented for you (see below). You are not at all obigated to follow this structure or use the code given. \n",
    "\n",
    "**Notes:** \n",
    "- When referring to a \"polynomial\", we either refer to an image or to the 4 coefficients. The meaning should be clear from context  \n",
    "- \"network\" and \"model\" are used interchangeably throughout this documentation.\n",
    " \n",
    " \n",
    "### **General structure (in order):**\n",
    " 1. DataSet and DataLoader            - this is how you load data into the network **(given to you)**\n",
    " 2. Seperable convolution class       - You can use this in your network class if you want **(given to you)**\n",
    " 3. Neural Network definition         - the class that defines your model   **(NEED TO IMPLEMENT)**\n",
    " 4. calculate model parameters number - useful to get a feel for network's size **(given to you)**\n",
    " 5. estimate number of ops per forward feed - the bigger this is the slower the training **(given to you)**\n",
    " 6. loss function definition          - to be used in training **(NEED TO IMPLEMENT)**\n",
    " 7. create an optimizer               - choose your optimizer **(NEED TO IMPLEMENT)**\n",
    " 8. estimate ops per forward feed     - use function given to you to estimate this **(implement for convenience)**\n",
    " 9. view images and coefficients      - example code to help you see how to use loaders **(given to you)**\n",
    " 10. validate_model                   - returns avg loss per image for a model and loader  **(NEED TO IMPLEMENT)**\n",
    " 11. train_model                      - trains the network **(NEED TO IMPLEMENT)**\n",
    " 12. plot train/validaion losses      - visualizing train and validation losses from training **(given to you)**\n",
    " 13. save/load model                  - allows you to save and load model to disk for later use **(given to you)**\n",
    " 14. visualizing polynomials          - overlaying net polynomials over actual polynomials **(given to you)**\n",
    " \n",
    " \n",
    "### ** What you need to do **\n",
    "You have 5 (and a half...) things to do:\n",
    " 1. Create a PolyNet class, which is your network model. This is a key component. The output of your model should be 4 numbers that represent the coefficients of the input polynomial image fed into your network. You should keep in mind the number of parameters in your model. If there are too many, it may overfit (and take longer to run). If there are too few, it may not be able to learn the task needed. A typical structure would have convolutional layers first and fully connected at the end, thus reducing number of parameteres. Consider which activation function you want to use, and whether or not you wish to use batch normalization or dropout. Also, you may use seperable convolution or regular convolution or both. Maxpool layers are also possible. Be creative.\n",
    " \n",
    " 2. Create a loss function. This is another key component as it defines what it means for two polynomials to be similar or not. Namely, two polynomials (represented by two sets of coefficients: ground truth (\"labels\") and network outputs (\"outputs\")), whose images look similar should have a smaller loss than two polynomials whose images look less simialr. Think about how you would quantify \"closeness\"/\"similarity\" of polynomials.\n",
    "\n",
    " 3. Choose an optimizer. Look here for some ideas: https://pytorch.org/docs/master/optim.html\n",
    " \n",
    " 4. (This is the \"half\" thing to do). For your conveinece you may want to use the function calc_ops() that is given to you to calculate an estimate of the number of operations that your network does per feed forward. To do this, you need to enter your own network structure. An example of an arbitrary network is privided to you.\n",
    " \n",
    " 5. Create a validate_model function that assesses (tests) the performance of a model. It returns the average loss per image for a given loader. It can be run on any set of data (train, validation, or test). You may want to run this function on validation set (loader) from within the train function (see below) after each epoch so as to see how loss behaves on validation during the training process.\n",
    " \n",
    " 6. Create a train_model function that trains model. This function updates the following parameteres: model, train_losses, and validation_losses. Model is updated simply by the training processes when optimzer.step() is called. The other two parameteres are lists that hold the average loss per image for the corresponding data (train or validation). After every epoch (i.e., iteration that goes over the entire train data), you should save into these lists the average loss per image for the corresponding data loader. These lists are useful in that you can plot them (functionality given to you) and observe how model behaves. Observe: this fucnton returs nothing, however, it updates parameteres by reference. Specifically, model is updated (trained), and so are train and validation losses values.\n",
    " \n",
    "**Note:** You are given three sets of data: trian, validation, and test. It is recommended that test not be touched until the very end, and validation be used to get a sense of your network's performance. \n",
    "\n",
    "### ** Running on a GPU: **\n",
    "It is not necessary to use a GPU for this project. However, if you choose to do so, you will gain a major speedup to your training, which will save you much time. The code given to you identifies the hardware used and will  automatically run on either a GPU or CPU. \n",
    "\n",
    "### ** Useful links: **\n",
    "1. PyTorch master tutorial - VERY useful: https://pytorch.org/docs/master/nn.html\n",
    "2. PyTorch optimizers: https://pytorch.org/docs/master/optim.html\n",
    "3. A list of possible reasons why things go wrong: https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de\n",
    "\n",
    "### ** Final tips: **\n",
    "Use the Internet! Things will not work first time. You will get strange error messages. Google them up. The web is  great resource for tackling problems ranging from python error messages, to things not doing what you'd like them to do.\n",
    "\n",
    "\n",
    "### ** Submission Instructions**\n",
    "The project is to be submittd in pairs. You need to submit the following three files:\n",
    "1. model.dat                            - This is your saved model \n",
    "2. Polynomial_Identifier_Project.ipynb  - This is this notebook containing all of your work\n",
    "3. model.py                             - A file containing your PolyNet class only. \n",
    "\n",
    "Before you submit, run \"check_before_submission.ipynb\" to make sure your model can be properly tested. See instructions for running this notebook inside.\n",
    "\n",
    "Make sure your names appear at the top of this notebook in the appropriate place.\n",
    "\n",
    "### **GOOD LUCK!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from PIL import ImageDraw\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from scipy import integrate as integrate\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class definition (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Initializing dataset by generating a dicitonary of labels, where an image file name is the key \n",
    "        and its labels are the contents of that entry in the dictionary. Images are not loaded. This way it\n",
    "        is possible to iterate over arbitrarily large datasets (limited by labels dicitonary fitting \n",
    "        in memory, which is not a problem in practice)\n",
    "        \n",
    "        Args:\n",
    "            dataset_dir : path to directory with images and labels. In this directory we expect to find\n",
    "                          a directory called \"images\" containing the input images, and a file called \n",
    "                          \"labels.txt\" containing desired labels (coefficients)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dict = self.gen_labels_dict()\n",
    "        self.images_keys = list(self.labels_dict)  # getting the keys of the dictionary as list\n",
    "        self.images_keys.sort()                    # sorting so as to have in alphabetical order \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_dict)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        \"\"\"\n",
    "        This funtion makes it possible to iterate over the PolyDataset\n",
    "        Args:\n",
    "            index: running index of images\n",
    "            \n",
    "        Returns:\n",
    "            sample: a dicitionary with three entries:\n",
    "                    1. 'image'  contains the image\n",
    "                    2. 'labels' contains labels (coeffs) corresponding to image\n",
    "                    3. 'fname'  contains name of file (image_key) - may be useful for debugging\n",
    "        \"\"\"\n",
    "        image_key = self.images_keys[index]     # recall - key is the file name of the corresponding image\n",
    "        image = np.array(Image.open(image_key)) # image has shape: (128, 128, 3)\n",
    "        image = image/255.0                     # simple normalization - just to maintain small numbers\n",
    "        image = np.transpose(image, (2, 0, 1))  # network needs RGB channels to be first index\n",
    "        labels = self.labels_dict[image_key]\n",
    "        sample = {'image': image, 'labels': labels, 'fname':image_key}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def gen_labels_dict(self):\n",
    "        \"\"\"\n",
    "        This fucntion generates a dictionary of labels\n",
    "        \n",
    "        Returns:\n",
    "            labels_dict: the key is image file name and an array of labels is the corresponding contents \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_fname = self.dataset_dir + \"/labels.txt\"\n",
    "        labels_dict = {}\n",
    "        with open(labels_fname, \"r\") as inp:\n",
    "            for line in inp:\n",
    "                line = line.split('\\n')[0]                                      # remove '\\n' from end of line \n",
    "                line = line.split(',')\n",
    "                key  = self.dataset_dir + '/images/' + line[0].strip() + \".png\" # image file name is the key\n",
    "                del line[0]\n",
    "                \n",
    "                list_from_line = [float(item) for item in line]\n",
    "                labels_dict[key] = np.asarray(list_from_line, dtype=np.float32)\n",
    "                        \n",
    "        return labels_dict             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader examples     : ~ 1008\n",
      "validation loader examples: ~ 100\n",
      "test loader examples      : ~ 100\n"
     ]
    }
   ],
   "source": [
    "train_dir      = \"./train/\"\n",
    "validation_dir = \"./validation/\"\n",
    "test_dir       = \"./test/\"\n",
    "\n",
    "\n",
    "train_dataset = PolyDataset(train_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=16,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = PolyDataset(validation_dir)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = PolyDataset(test_dir)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=1,\n",
    "                          shuffle=False)\n",
    "import matplotlib.pyplot as plt \n",
    "conaiter =  iter(train_loader).next()\n",
    "#print(conaiter[\"image\"].shape)\n",
    "#print(conaiter[\"labels\"])\n",
    "\n",
    "print(\"train loader examples     : ~\", len(train_loader)*train_loader.batch_size)\n",
    "print(\"validation loader examples: ~\", len(validation_loader)*validation_loader.batch_size)\n",
    "print(\"test loader examples      : ~\", len(test_loader)*test_loader.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperable convolution class (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Seperable convolution - you can try it out if you want\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConv2d,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels,\n",
    "                               in_channels,\n",
    "                               kernel_size,\n",
    "                               stride,\n",
    "                               padding,\n",
    "                               dilation,\n",
    "                               groups=in_channels,\n",
    "                               bias=bias)\n",
    "        \n",
    "        self.pointwise = nn.Conv2d(in_channels, \n",
    "                                   out_channels, \n",
    "                                   kernel_size=1,\n",
    "                                   stride=1, \n",
    "                                   padding=0, \n",
    "                                   dilation=1, \n",
    "                                   groups=1, \n",
    "                                   bias=bias)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolyNet(nn.Module):    # nn.Module is parent class\n",
    "    def __init__(self,p=0.3):\n",
    "        super(PolyNet, self).__init__()  # calls init of parent class\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=3,\n",
    "                                              out_channels=6,\n",
    "                                              kernel_size=5),\n",
    "                                     nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(6),\n",
    "                                    nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(16),\n",
    "                                    nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=16,\n",
    "                                              out_channels=20,\n",
    "                                              kernel_size=5,\n",
    "                                              stride=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.BatchNorm2d(20))\n",
    "                                   \n",
    "\n",
    "        self.fc1 = nn.Linear(20*25*25, 120, bias=True)\n",
    "        self.fc3 = nn.Linear(120, 4, bias=True)\n",
    "        # ----------------------------------------------\n",
    "        # implementation needed here\n",
    "        # ----------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc3(out)\n",
    "        \"\"\"\n",
    "        Feed forward through network\n",
    "        Args:\n",
    "            x - input to the network\n",
    "            \n",
    "        Returns \"out\", which is the network's output\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------------------------\n",
    "        # implementation needed here\n",
    "        # ----------------------------------------------\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get number of trainable parameters (given to you)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_params_num(model):\n",
    "    \"\"\"\n",
    "    This fucntion returns the number of trainable parameters of neural network model\n",
    "    You may want to call it after you create your model to see how many parameteres the model has\n",
    "    Args:\n",
    "        model - neural net to examine. NOTE: this is an instantiation of the PolyNet class, not the class itself \n",
    "    \"\"\"\n",
    "    \n",
    "    model_parameters = filter(lambda p: p.requires_grad==True, model.parameters())\n",
    "    params_num = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an estimate number of operations (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ops(inp_size, net_struct):\n",
    "    \"\"\"\n",
    "    Calculates a rough number of operations for a given network topology\n",
    "    Args:\n",
    "        inp_size - (W,H) of input \n",
    "        net_struct - list of tuples describing structure of network. \n",
    "        \n",
    "        Example:\n",
    "         (('conv2d', (3, 8, 3, 1, 0)),  # cin, cout, kernel, stride, pad\n",
    "          ('conv2d', (8, 8, 3, 1, 0)),\n",
    "          ('MaxPool2d', (2,2)),         # kernel, stride\n",
    "          ('fc', (64, 8)),      \n",
    "          ('fc', (8, 4)))\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    ops = 0\n",
    "    W, H = inp_size\n",
    "    for curr_item in net_struct:\n",
    "        if curr_item[0] == 'conv2d':\n",
    "            cin = curr_item[1][0]\n",
    "            cout = curr_item[1][1]\n",
    "            kernel = curr_item[1][2]\n",
    "            stride = curr_item[1][3]\n",
    "            curr_ops = (W*H*cin*cout*kernel*kernel)/stride\n",
    "            pad = curr_item[1][4]\n",
    "            W = (W +2*pad - kernel)/stride + 1\n",
    "            H = (H +2*pad - kernel)/stride + 1\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "        elif curr_item[0] == 'MaxPool2d':\n",
    "            kernel = curr_item[1][0]\n",
    "            stride = curr_item[1][1]\n",
    "            W = (W - kernel)/stride + 1\n",
    "            H = (H - kernel)/stride + 1\n",
    "        else:\n",
    "            curr_ops = curr_item[1][0] * curr_item[1][1]\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "            \n",
    "    return int(ops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a,b,c,d):  #a is a parameter, x is the variable I want to integrate over\n",
    "    return a*np.power(x, 3)+b*np.power(x, 2)+ c*x+d\n",
    "\n",
    "def my_loss(outputs, labels):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        outputs - output of network ([batch size, polynomials rank + 1 (i.e., coefficients number)]) \n",
    "        labels  - desired coefficients  ([batch size, polynomials rank + 1 (i.e., coefficients number)])\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "    loss = loss.to(device)\n",
    "    o= outputs.cpu().data.numpy()\n",
    "    l= labels.cpu().data.numpy()\n",
    "    polyList = o-l\n",
    "    for inx,poly in enumerate(polyList):\n",
    "        result = integrate.quad(f,-50,200,args=(poly[0],poly[1],poly[2],poly[3]))\n",
    "        loss = loss + abs(result[0])\n",
    "                                      \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and choice of optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model trainable parameters: 1511580\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "        \n",
    "model = PolyNet().to(device)\n",
    "model.apply(init_weights)\n",
    "print (\"Number of model trainable parameters:\", get_train_params_num(model))\n",
    "\n",
    "#----------------------------------------------\n",
    "#  Choose your optimizer:\n",
    "#  implementation needed here \n",
    "#----------------------------------------------\n",
    "# learning_rate = 0.01\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.75,nesterov=True,weight_decay=0.001)\n",
    "# optimizer= torch.optim.Adam(model.parameters(),lr=learning_rate ,weight_decay=0.00001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "# model = models.resnet18()\n",
    "\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc =nn.Linear(num_ftrs, 4, bias=True)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9,nesterov=True,weight_decay=0.00001)\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr=0.001 ,weight_decay=0.001)\n",
    "\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.09)\n",
    "import copy\n",
    "best_loss = 10000000000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check rough number of ops for network (for your convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv2d', (3, 3, 3, 1, 1)) : 1,327,104\n",
      "('conv2d', (3, 3, 3, 1, 1)) : 331,776\n",
      "('fc', (2883, 4)) : 11,532\n",
      "\n",
      "Total ops: 1,670,412\n"
     ]
    }
   ],
   "source": [
    "inp_size = (128,128)\n",
    "\n",
    "example_net = (('conv2d', (3, 3, 3, 1, 1)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('conv2d', (3, 3, 3, 1, 1)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('fc', (2883, 4)))\n",
    "\n",
    "### USE YOUR OWN NETWORK ####\n",
    "              \n",
    "ops = calc_ops(inp_size, example_net)\n",
    "print()\n",
    "print(\"Total ops: {:,}\".format(ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View images, target coefficiets and  network coefficients (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing image:  ./train//images/0180.png\n",
      "Target coeffs : [0.36, 0.88, -1.67, 0.82]\n",
      "\n",
      "showing image:  ./train//images/0458.png\n",
      "Target coeffs : [2.51, -4.93, 2.11, 0.67]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View first image of a given number of batches assuming that model has been created. Currently, code lines assuming\n",
    "model has been creatd, are commented out. Without a model, you can view target coefficients and the corresponding\n",
    "images.\n",
    "This is given to you so that you may see how loaders and model can be used. \n",
    "\"\"\"\n",
    "\n",
    "loader = train_loader # choose from which loader to show images\n",
    "bacthes_to_show = 2\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(loader, 0): #0 means that counting starts at zero\n",
    "        inputs = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)\n",
    "  \n",
    "        img_fnames = data['fname']\n",
    "        #outputs = model(inputs.float())\n",
    "        img = Image.open(img_fnames[0])\n",
    "        \n",
    "        print (\"showing image: \", img_fnames[0])\n",
    "        \n",
    "        labels_np_arr = labels[0]   # using \".numpy()\" to convert tensor to numpy array\n",
    "        labels_str = [ float((\"{0:.2f}\".format(x))) for x in labels_np_arr]\n",
    "        \n",
    "        #outputs_np_arr = outputs[0] # using \".numpy()\" to convert tensor to numpy array\n",
    "        #outputs_str = [ float((\"{0:.2f}\".format(x))) for x in outputs_np_arr]\n",
    "        print(\"Target coeffs :\", labels_str )\n",
    "        #print(\"network coeffs:\", outputs_str)\n",
    "        print()\n",
    "        img.show()\n",
    "        \n",
    "        if (i+1) == bacthes_to_show:\n",
    "            break\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def validate_model(model, loader):\n",
    "#     \"\"\"\n",
    "#     This function parses a given loader and returns the avergae (per image) loss (as defined by \"my_loss\" \n",
    "#     of the entire dataset associated with the given loader.\n",
    "    \n",
    "#     Args:\n",
    "#         model  - neural network to examine\n",
    "#         loader - where input data comes from (train, validation, or test)\n",
    "        \n",
    "#     returns:\n",
    "#         average loss per image in variable named \"avg_loss\"\n",
    "#     \"\"\"\n",
    "#     avg_loss = 0\n",
    "#     for i, data in enumerate(loader): #0 means that counting starts at zero\n",
    "#         image = (data['image']).to(device)\n",
    "#         labels = (data['labels']).to(device)\n",
    "#         # Forward pass\n",
    "#         outputs = model(image.float())\n",
    "#         loss = my_loss(outputs, labels)\n",
    "#         avg_loss = avg_loss + loss\n",
    "        \n",
    "#     avg_loss = avg_loss / (len(loader)*loader.batch_size)\n",
    "#     return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_train(model,optimizer,scheduler,train_loader,train_losses):\n",
    "    scheduler.step()\n",
    "    model.train()  #back to default\n",
    "    avg_loss = 0\n",
    "    for i, data in enumerate(loader): #0 means that counting starts at zero\n",
    "            image = (data['image']).to(device)\n",
    "            labels = (data['labels']).to(device)\n",
    "            # Forward pass\n",
    "            with torch.set_grad_enabled(True): \n",
    "                outputs = model(image.float())\n",
    "                loss = my_loss(outputs, labels)\n",
    "                avg_loss = avg_loss + loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "    avg_loss = avg_loss / (len(train_loader)*train_loader.batch_size)   \n",
    "    train_losses.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_validation(model,optimizer,scheduler,validation_loader,validation_losses,best_model_wts):\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    global best_loss \n",
    "    avg_val_loss = 0\n",
    "    for i, data in enumerate(loader): #0 means that counting starts at zero\n",
    "        image = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.no_grad():    \n",
    "            outputs = model(image.float())\n",
    "            loss = my_loss(outputs, labels)\n",
    "            avg_val_loss = avg_val_loss + loss\n",
    " \n",
    "    avg_val_loss = avg_val_loss / (len(validation_loader)*validation_loader.batch_size)   \n",
    "    validation_losses.append(avg_val_loss)\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_wts=copy.deepcopy(model.state_dict())\n",
    " \n",
    "    return best_model_wts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                train_loader,\n",
    "                validation_loader,\n",
    "                train_losses,\n",
    "                validation_losses,\n",
    "                epochs=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a neural network. \n",
    "    Args;\n",
    "        model               - model to be trained\n",
    "        optimizer           - optimizer used for training\n",
    "        train_loader        - loader from which data for training comes \n",
    "        validation_loader   - loader from which data for validation comes (maybe at the end, you use test_loader)\n",
    "        train_losses        - adding train loss value after each epoch to this list for future analysis\n",
    "        validation_losses   - adding validation loss value after each epoch to this list for future analysis\n",
    "        epochs              - number of runs over the entire data set \n",
    "        \n",
    "    No value is returned - model is updated during training \n",
    "    \n",
    "    \"\"\"\n",
    "    best_model_wts=copy.deepcopy(model.state_dict())    \n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch,epochs-1))\n",
    "        phase_train(model,optimizer,scheduler,train_loader,train_losses)\n",
    "        best_model_wts = phase_validation(model,optimizer,scheduler,validation_loader,validation_losses,best_model_wts)\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual train (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/14\n",
      "Epoch 1/14\n",
      "Epoch 2/14\n",
      "Epoch 3/14\n",
      "Epoch 4/14\n",
      "Epoch 5/14\n",
      "Epoch 6/14\n",
      "Epoch 7/14\n",
      "Epoch 8/14\n",
      "Epoch 9/14\n",
      "Epoch 10/14\n",
      "Epoch 11/14\n",
      "Epoch 12/14\n",
      "Epoch 13/14\n",
      "Epoch 14/14\n"
     ]
    }
   ],
   "source": [
    "# Using two lists (train_losses, validation_losses) containing history of losses (i.e., loss for each \n",
    "# training epoch) for train and validation sets. If thess are not defined, we define them. Otherwise, the \n",
    "# function train_model updates these two lists (by adding loss values after each epoch when it is called for\n",
    "# further training) in order to be able to visualize train and validation losses\n",
    "\n",
    "\n",
    "if not 'train_losses' in vars():\n",
    "    train_losses = []\n",
    "if not 'validation_losses' in vars():\n",
    "    validation_losses = []\n",
    "\n",
    "\n",
    "model = train_model(model, \n",
    "            optimizer,\n",
    "            exp_lr_scheduler,\n",
    "            train_loader, \n",
    "            validation_loader, \n",
    "            train_losses, \n",
    "            validation_losses,\n",
    "            epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses from training process (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iteration = np.arange(0., len(train_losses))\n",
    "plt.plot(iteration, train_losses, 'g-', iteration, validation_losses, 'r-')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Model (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, train_losses, validation_losses, save_dir):\n",
    "    \"\"\"\n",
    "    saving model, train losses, and validation losses\n",
    "    Args:\n",
    "        model              - NN to be saved\n",
    "        train_losses       - history of losses for training dataset\n",
    "        validation_losses  - history of losses for validation dataset\n",
    "        save_dir           - directory where to save the above\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    torch.save(model, save_dir + \"/model.dat\")\n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"wt\")\n",
    "    train_losses_f.writelines( \"%.3f\\n\" % item for item in train_losses)\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"wt\")\n",
    "    validation_losses_f.writelines( \"%.3f\\n\" % item for item in validation_losses)\n",
    "\n",
    "    return\n",
    "   \n",
    "\n",
    "def load(save_dir):\n",
    "    \"\"\"\n",
    "    loading model, train losses, and validation losses\n",
    "    Args:\n",
    "       save_dir  - dir name from where to load \n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.load(save_dir + \"/model.dat\") \n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"rt\")\n",
    "    train_losses    = train_losses_f.readlines()\n",
    "    train_losses   = [float(num) for num in train_losses]\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"rt\")\n",
    "    validation_losses   = validation_losses_f.readlines()\n",
    "    validation_losses   = [float(num) for num in validation_losses]\n",
    "    \n",
    "    return (model, train_losses, validation_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type PolyNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Create a directory \"./saves/\" where you place your saved models\n",
    "\n",
    "save(model, train_losses, validation_losses, \"./saves/try/\")\n",
    "\n",
    "model, train_losses, validation_losses = load(\"./saves/try/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw target and net polynomials (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_points_dict_for_all_images(model, loader): \n",
    "    \"\"\"\n",
    "    create a dictionary of dictionaries, where main key is file name of image.\n",
    "    Each elemnts is then a dictionary of two items: 'pts_net', 'pts_tgt'\n",
    "    each such item is a list of points of the form: [[x1,y1], [x2,y2],....]\n",
    "    \n",
    "    Args:\n",
    "        model     - network for which polynimials are examined\n",
    "        loader    - input data to use \n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    points_dict = {}  \n",
    "        \n",
    "    k=0\n",
    "    print (\"generating points_dict for all images:\")\n",
    "    \n",
    "    for data in loader:\n",
    "        # get inputs\n",
    "        inputs = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)\n",
    "        img_fnames = data['fname'] \n",
    "      \n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs.float())\n",
    "        curr_batch_size = np.shape(outputs)[0]\n",
    "        coeffs_num = np.shape(labels)[1]  # labels shape is (batch_size, coeffs_num)\n",
    "        image_size = np.shape(inputs[0])  # image_size = [3, w, h]\n",
    "        _, width, height = image_size\n",
    "        \n",
    "        for i in range (curr_batch_size): \n",
    "            coeffs_net = [] \n",
    "            coeffs_tgt = []\n",
    "            for curr_coeff in range(coeffs_num):\n",
    "                coeffs_net.append(outputs[i, curr_coeff].item()) \n",
    "                coeffs_tgt.append(labels[i, curr_coeff].item())\n",
    "                \n",
    "\n",
    "            poly_net = np.poly1d(coeffs_net) # generate polynomial representation\n",
    "            poly_tgt = np.poly1d(coeffs_tgt) # generate polynomial representation\n",
    "\n",
    "    \n",
    "            x_pts = np.arange(0.0, 1.0, 0.01/width)\n",
    "            y_pts_net = poly_net(x_pts)  # execute net polynomial on x_pts\n",
    "            y_pts_tgt = poly_tgt(x_pts)  # execute tgt polynomial on x_pts\n",
    "            \n",
    "            y_pts_net = 1 - y_pts_net  # b/c on screen y=0 is on top, now 0 is at bottom as we are used\n",
    "            y_pts_tgt = 1 - y_pts_tgt  # b/c on screen y=0 is on top, now 0 is at bottom as we are used\n",
    "    \n",
    "            x_pts *= width\n",
    "            y_pts_net *= height\n",
    "            y_pts_tgt *= height\n",
    "    \n",
    "            \n",
    "            pts_net = [list(a) for a in  zip(x_pts, y_pts_net)]\n",
    "            pts_tgt = [list(a) for a in  zip(x_pts, y_pts_tgt)]\n",
    "\n",
    "            input_fname = img_fnames[i] \n",
    "            k+=1\n",
    "            print (str(k) + \".   \" + input_fname)\n",
    "            points_dict[input_fname] = {'pts_net': pts_net,\n",
    "                                        'pts_tgt': pts_tgt} \n",
    "   \n",
    "    model.train()  #back to default           \n",
    "    return points_dict\n",
    "\n",
    "\n",
    "def draw_poly(points_dict, out_dir):\n",
    "    \"\"\"\n",
    "    Draws polynomials from given dictionary of dictionaries \n",
    "    \n",
    "    Args:\n",
    "        points_dict - dictionary. key is file name of frame (fname) and elemet is a dictionary with\n",
    "                       keys: 'pts_net', 'pts_tgt'. Each of which has a list of points:(x,y)\n",
    "        out_dir     - ouptut directory name (e.g.: 'draws/')\n",
    "    \"\"\"\n",
    "    \n",
    "       \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    files = glob.glob(out_dir + '*.png')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "\n",
    "\n",
    "    print(\"Marking frames:\")\n",
    "    index = 1\n",
    "    for fname in points_dict:\n",
    "        orig_img = Image.open(fname)\n",
    "        img = Image.new(orig_img.mode, orig_img.size, (255, 255, 255))\n",
    "        \n",
    "        fname_points_list_net = points_dict[fname]['pts_net']\n",
    "        fname_points_list_tgt = points_dict[fname]['pts_tgt']\n",
    "\n",
    "        fname_points_list_net = [(point[0], point[1]) for point in fname_points_list_net]\n",
    "        fname_points_list_tgt = [(point[0], point[1]) for point in fname_points_list_tgt]\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.point(fname_points_list_net, (255, 0, 0, 255))  # net: red\n",
    "        draw.point(fname_points_list_tgt, (0, 255, 0, 255))  # tgt: green\n",
    "        \n",
    "    \n",
    "        img.save(out_dir + fname.split('/')[-1])\n",
    "        \n",
    "        print (index, out_dir + fname.split('/')[-1])\n",
    "        index+=1\n",
    "        \n",
    "        \n",
    "def draw_loader_polynomials(model, loader, out_dir):\n",
    "    \"\"\"\n",
    "    This fucntion receives a model and a loader and for each polymonial image in the loader it draws\n",
    "    the polynomial that it identifies. Both polynomials (the original (in green) and the network's (in red)) \n",
    "    are saved as an image in the given out_dir diretory.\n",
    "    Args:\n",
    "        model   - network for which polynimials are drawn\n",
    "        loader  - input data to use \n",
    "        out_dir - ouptut directory name (e.g.: 'draws/')   \n",
    "    \"\"\"\n",
    "    \n",
    "    points_dict = gen_points_dict_for_all_images(model, loader)  \n",
    "    draw_poly(points_dict, out_dir)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example of how to draw target and net polynomials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating points_dict for all images:\n",
      "1.   ./validation//images/0000.png\n",
      "2.   ./validation//images/0001.png\n",
      "3.   ./validation//images/0002.png\n",
      "4.   ./validation//images/0003.png\n",
      "5.   ./validation//images/0004.png\n",
      "6.   ./validation//images/0005.png\n",
      "7.   ./validation//images/0006.png\n",
      "8.   ./validation//images/0007.png\n",
      "9.   ./validation//images/0008.png\n",
      "10.   ./validation//images/0009.png\n",
      "11.   ./validation//images/0010.png\n",
      "12.   ./validation//images/0011.png\n",
      "13.   ./validation//images/0012.png\n",
      "14.   ./validation//images/0013.png\n",
      "15.   ./validation//images/0014.png\n",
      "16.   ./validation//images/0015.png\n",
      "17.   ./validation//images/0016.png\n",
      "18.   ./validation//images/0017.png\n",
      "19.   ./validation//images/0018.png\n",
      "20.   ./validation//images/0019.png\n",
      "21.   ./validation//images/0020.png\n",
      "22.   ./validation//images/0021.png\n",
      "23.   ./validation//images/0022.png\n",
      "24.   ./validation//images/0023.png\n",
      "25.   ./validation//images/0024.png\n",
      "26.   ./validation//images/0025.png\n",
      "27.   ./validation//images/0026.png\n",
      "28.   ./validation//images/0027.png\n",
      "29.   ./validation//images/0028.png\n",
      "30.   ./validation//images/0029.png\n",
      "31.   ./validation//images/0030.png\n",
      "32.   ./validation//images/0031.png\n",
      "33.   ./validation//images/0032.png\n",
      "34.   ./validation//images/0033.png\n",
      "35.   ./validation//images/0034.png\n",
      "36.   ./validation//images/0035.png\n",
      "37.   ./validation//images/0036.png\n",
      "38.   ./validation//images/0037.png\n",
      "39.   ./validation//images/0038.png\n",
      "40.   ./validation//images/0039.png\n",
      "41.   ./validation//images/0040.png\n",
      "42.   ./validation//images/0041.png\n",
      "43.   ./validation//images/0042.png\n",
      "44.   ./validation//images/0043.png\n",
      "45.   ./validation//images/0044.png\n",
      "46.   ./validation//images/0045.png\n",
      "47.   ./validation//images/0046.png\n",
      "48.   ./validation//images/0047.png\n",
      "49.   ./validation//images/0048.png\n",
      "50.   ./validation//images/0049.png\n",
      "51.   ./validation//images/0050.png\n",
      "52.   ./validation//images/0051.png\n",
      "53.   ./validation//images/0052.png\n",
      "54.   ./validation//images/0053.png\n",
      "55.   ./validation//images/0054.png\n",
      "56.   ./validation//images/0055.png\n",
      "57.   ./validation//images/0056.png\n",
      "58.   ./validation//images/0057.png\n",
      "59.   ./validation//images/0058.png\n",
      "60.   ./validation//images/0059.png\n",
      "61.   ./validation//images/0060.png\n",
      "62.   ./validation//images/0061.png\n",
      "63.   ./validation//images/0062.png\n",
      "64.   ./validation//images/0063.png\n",
      "65.   ./validation//images/0064.png\n",
      "66.   ./validation//images/0065.png\n",
      "67.   ./validation//images/0066.png\n",
      "68.   ./validation//images/0067.png\n",
      "69.   ./validation//images/0068.png\n",
      "70.   ./validation//images/0069.png\n",
      "71.   ./validation//images/0070.png\n",
      "72.   ./validation//images/0071.png\n",
      "73.   ./validation//images/0072.png\n",
      "74.   ./validation//images/0073.png\n",
      "75.   ./validation//images/0074.png\n",
      "76.   ./validation//images/0075.png\n",
      "77.   ./validation//images/0076.png\n",
      "78.   ./validation//images/0077.png\n",
      "79.   ./validation//images/0078.png\n",
      "80.   ./validation//images/0079.png\n",
      "81.   ./validation//images/0080.png\n",
      "82.   ./validation//images/0081.png\n",
      "83.   ./validation//images/0082.png\n",
      "84.   ./validation//images/0083.png\n",
      "85.   ./validation//images/0084.png\n",
      "86.   ./validation//images/0085.png\n",
      "87.   ./validation//images/0086.png\n",
      "88.   ./validation//images/0087.png\n",
      "89.   ./validation//images/0088.png\n",
      "90.   ./validation//images/0089.png\n",
      "91.   ./validation//images/0090.png\n",
      "92.   ./validation//images/0091.png\n",
      "93.   ./validation//images/0092.png\n",
      "94.   ./validation//images/0093.png\n",
      "95.   ./validation//images/0094.png\n",
      "96.   ./validation//images/0095.png\n",
      "97.   ./validation//images/0096.png\n",
      "98.   ./validation//images/0097.png\n",
      "99.   ./validation//images/0098.png\n",
      "100.   ./validation//images/0099.png\n",
      "Marking frames:\n",
      "1 ./validation/draw/0000.png\n",
      "2 ./validation/draw/0001.png\n",
      "3 ./validation/draw/0002.png\n",
      "4 ./validation/draw/0003.png\n",
      "5 ./validation/draw/0004.png\n",
      "6 ./validation/draw/0005.png\n",
      "7 ./validation/draw/0006.png\n",
      "8 ./validation/draw/0007.png\n",
      "9 ./validation/draw/0008.png\n",
      "10 ./validation/draw/0009.png\n",
      "11 ./validation/draw/0010.png\n",
      "12 ./validation/draw/0011.png\n",
      "13 ./validation/draw/0012.png\n",
      "14 ./validation/draw/0013.png\n",
      "15 ./validation/draw/0014.png\n",
      "16 ./validation/draw/0015.png\n",
      "17 ./validation/draw/0016.png\n",
      "18 ./validation/draw/0017.png\n",
      "19 ./validation/draw/0018.png\n",
      "20 ./validation/draw/0019.png\n",
      "21 ./validation/draw/0020.png\n",
      "22 ./validation/draw/0021.png\n",
      "23 ./validation/draw/0022.png\n",
      "24 ./validation/draw/0023.png\n",
      "25 ./validation/draw/0024.png\n",
      "26 ./validation/draw/0025.png\n",
      "27 ./validation/draw/0026.png\n",
      "28 ./validation/draw/0027.png\n",
      "29 ./validation/draw/0028.png\n",
      "30 ./validation/draw/0029.png\n",
      "31 ./validation/draw/0030.png\n",
      "32 ./validation/draw/0031.png\n",
      "33 ./validation/draw/0032.png\n",
      "34 ./validation/draw/0033.png\n",
      "35 ./validation/draw/0034.png\n",
      "36 ./validation/draw/0035.png\n",
      "37 ./validation/draw/0036.png\n",
      "38 ./validation/draw/0037.png\n",
      "39 ./validation/draw/0038.png\n",
      "40 ./validation/draw/0039.png\n",
      "41 ./validation/draw/0040.png\n",
      "42 ./validation/draw/0041.png\n",
      "43 ./validation/draw/0042.png\n",
      "44 ./validation/draw/0043.png\n",
      "45 ./validation/draw/0044.png\n",
      "46 ./validation/draw/0045.png\n",
      "47 ./validation/draw/0046.png\n",
      "48 ./validation/draw/0047.png\n",
      "49 ./validation/draw/0048.png\n",
      "50 ./validation/draw/0049.png\n",
      "51 ./validation/draw/0050.png\n",
      "52 ./validation/draw/0051.png\n",
      "53 ./validation/draw/0052.png\n",
      "54 ./validation/draw/0053.png\n",
      "55 ./validation/draw/0054.png\n",
      "56 ./validation/draw/0055.png\n",
      "57 ./validation/draw/0056.png\n",
      "58 ./validation/draw/0057.png\n",
      "59 ./validation/draw/0058.png\n",
      "60 ./validation/draw/0059.png\n",
      "61 ./validation/draw/0060.png\n",
      "62 ./validation/draw/0061.png\n",
      "63 ./validation/draw/0062.png\n",
      "64 ./validation/draw/0063.png\n",
      "65 ./validation/draw/0064.png\n",
      "66 ./validation/draw/0065.png\n",
      "67 ./validation/draw/0066.png\n",
      "68 ./validation/draw/0067.png\n",
      "69 ./validation/draw/0068.png\n",
      "70 ./validation/draw/0069.png\n",
      "71 ./validation/draw/0070.png\n",
      "72 ./validation/draw/0071.png\n",
      "73 ./validation/draw/0072.png\n",
      "74 ./validation/draw/0073.png\n",
      "75 ./validation/draw/0074.png\n",
      "76 ./validation/draw/0075.png\n",
      "77 ./validation/draw/0076.png\n",
      "78 ./validation/draw/0077.png\n",
      "79 ./validation/draw/0078.png\n",
      "80 ./validation/draw/0079.png\n",
      "81 ./validation/draw/0080.png\n",
      "82 ./validation/draw/0081.png\n",
      "83 ./validation/draw/0082.png\n",
      "84 ./validation/draw/0083.png\n",
      "85 ./validation/draw/0084.png\n",
      "86 ./validation/draw/0085.png\n",
      "87 ./validation/draw/0086.png\n",
      "88 ./validation/draw/0087.png\n",
      "89 ./validation/draw/0088.png\n",
      "90 ./validation/draw/0089.png\n",
      "91 ./validation/draw/0090.png\n",
      "92 ./validation/draw/0091.png\n",
      "93 ./validation/draw/0092.png\n",
      "94 ./validation/draw/0093.png\n",
      "95 ./validation/draw/0094.png\n",
      "96 ./validation/draw/0095.png\n",
      "97 ./validation/draw/0096.png\n",
      "98 ./validation/draw/0097.png\n",
      "99 ./validation/draw/0098.png\n",
      "100 ./validation/draw/0099.png\n"
     ]
    }
   ],
   "source": [
    "# Drawing polynomials from validation loader and placing them in directory \"./validation/draw/\"\n",
    "# The green colored polynomials are the desired outputs and the red polynomials are network's \n",
    "# actual outputs. Notice that if in an image you see no red polynomial, it means it is not inside the \n",
    "# [0,1]x[0,1] \"box\", which is the domain considered.  This means that your model has siginificant error\n",
    "\n",
    "draw_loader_polynomials(model, validation_loader, './validation/draw/')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
